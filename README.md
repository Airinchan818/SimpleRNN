# SimpleRNN

Simple RNN from scractch this for education how SimpleRNN/VanillaRNN Layers can do forward and backward pass 
for NLP task 

what include inside:
-----------------------
- SimpleRNN class / Algorithym
- Embedding class / Algoritym
- Linear class / Algorithym
- ReLU class / Algorithym
- Sigmoid class / Algorithym
- BinaryCrossEntropy class Algorithym
note : all class included has with backpropogation algorithym, so is just plug and play

what Optimizer used ?
------------------------
AdamW(Adaptive Momentum Estimination) Weight decay, this optimizers choiced for solving gradient exploaded problem. it a crucial problem at RNN series layers 
like LSTM,GRU or SimpleRNN. 
this optimizer work do penalty at gradient  by sparated task with update weight mechanism 

suport my journey for more knowlege about deep learning:
kofi : https://ko-fi.com/alpin92578

credit:
----------------------------
datasets from kaggle users: Md. Ismiel Hossen Abir
with link: https://www.kaggle.com/datasets/mdismielhossenabir/sentiment-analysis 
license : MIT 

Author:
-----------------------
Candra Alpin Gunawan as programmers code and tester code 

contact:
email : hinamatsuriairin@gmail.com 

Note:
if some requlation at datasets or detail has changed please to contact me 
